{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In this paper we model the character classification and a simple version of language drift based on a popular TV show \"Friends\" as well as other shows running in the same era. The modeling methods applied in this paer include word2vec, DNN, CNN and LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<img src=\"friends-reunion-series-ftr.jpg\" style=\"width: 400px;\"> \n",
    "\n",
    "\"Friends\" is one of the most popular TV shows ran from 1994 to 2004 and we are big fans of \"Friends\" as well. There are 6 main charatcers in \"Friends\" who are Ross, Rachel, Joey, Chandler, Monica and Phoebe. While we watching the show, we both noticed that each character has a distinct personality and of course, style of speech, and it sometimes makes it very easy to tell who the characters are from their lines. Another interesting trend we noticed is the language drift. The show ran from 1994 to 2004, which was the 10 years that the world drastically changed, and so did the language. In this paper we will show the classification of the characters based on their lines, and briefly talk about the language drifts based on the show.\n",
    "\n",
    "*A \"Friends\" icon painting by Aysa:*\n",
    "\n",
    "<img src=\"friends_painting.jpg\" style=\"width: 200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "\n",
    "* Modeling using word2vec, DNN, CNN and LSTM models, the folders and scripts are shown as below. We are still looking to further improve the accuracy, if it's possible. \n",
    "* The baseline accuracy would be random guessing, which has an accuracy of 1/6, or 16.67%. If we get a higher accuracy in our modeling, the model is (at least somewhat) useful!\n",
    "* We have attched part of the codes in this report, the complete version of all codes will be uploaded into this repository: https://github.com/aysafanxm/w266-final-project\n",
    "\n",
    "\n",
    "### Folders\n",
    "* Model folder: word2vec model and DNN model\n",
    "* Cnnmodel folder: CNN model\n",
    "* Rnnmodel folder: LSTM model\n",
    "\n",
    "\n",
    "### Scripts\n",
    "* *handle_json.py* - The original lines.json file contains all lines from the show \"Friends\", which was in JSON format, we converted the character names into numbers and write them into data/feature_raw.txt. We also pick only the 6 main characters' lines (6 main characters: Ross, Rachel, Joey, Chandler, Monica and Phoebe).\n",
    "\n",
    "\n",
    "* *extract_label_and_sentence.py* - Extract labels from data/feature_raw.txt and write them into data/label.txt, also extract the segmantations into data/sentence.txt.\n",
    "\n",
    "\n",
    "* *extract_feature.py* - Train word vectors using word2vec (4 dimensions) and calculate the feature vectors of each sentence (take the average of the word vectors in each sentence), then write feature vectors into data/feature.txt. This process is mainly for DNN training because CNN and LSTM use embedding which doesn't train word vector the same way.\n",
    "\n",
    "\n",
    "* *main_word2vec.py* - DNN with 3 hidden layers. The neuron numbers of each layer is 40, 20 and 10, respectively. The input dimension is 4 (4 features) and the output dimension is 6 (6 characters). The first 2 layersâ€˜ activation function is sigmoid and the last layer's is softmax. The learning rate is 0.0001 and there are 1000 iterations. Note that there is a parameter, is_train, in the model, if is_train is True, it starts to train a new model, otherwise it takes the trained model.\n",
    "\n",
    "\n",
    "* *main.py* - Similar to main_word2vec.py, but it is DNN with embedding.\n",
    "\n",
    "\n",
    "* *data_helpers.py* - Helps to batch process the data\n",
    "\n",
    "\n",
    "* *cnn_model.py* - CNN with an embedding layer (100 dimensions word vectos), a CNN layer, a pool layer and a softmax layer to output the probability of each label.\n",
    "\n",
    "\n",
    "* *textCNN.py* - It takes the cnn_model.py to train or test the lines data. It takes 90% of the lines for training and 10% of them for testing. *The accuracy of CNN is 28%~30%.* The learn rate is 0.0001.\n",
    "\n",
    "\n",
    "* *textRNN.py* - RNN with an embedding layer, a bi-lstm layer, a concat layer, a fully connected layer and a softmax layer. It takes 90% of the lines for training and 10% of them for testing. *The accuracy of RNN is 39%~40%.* The learn rate is 0.0001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some blocks of the codes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import data_helpers\n",
    "from cnn_model import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_word2vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading feature information...\n",
      "\n",
      "reading label information...\n",
      "\n",
      "57168.875\n",
      "43253.44\n",
      "42323.68\n",
      "41977.105\n",
      "41805.535\n",
      "41703.83\n",
      "41635.438\n",
      "41586.19\n",
      "41549.297\n",
      "41520.805\n",
      "41498.117\n",
      "41479.613\n",
      "41464.203\n",
      "41451.184\n",
      "41440.0\n",
      "41430.324\n",
      "41421.84\n",
      "41414.35\n",
      "41407.68\n",
      "41401.68\n",
      "41396.26\n",
      "41391.36\n",
      "41386.9\n",
      "41382.81\n",
      "41379.055\n",
      "41375.61\n",
      "41372.4\n",
      "41369.457\n",
      "41366.68\n",
      "41364.098\n",
      "41361.67\n",
      "41359.4\n",
      "41357.273\n",
      "41355.258\n",
      "41353.363\n",
      "41351.566\n",
      "41349.87\n",
      "41348.266\n",
      "41346.715\n",
      "41345.25\n",
      "41343.83\n",
      "41342.484\n",
      "41341.168\n",
      "41339.934\n",
      "41338.742\n",
      "41337.594\n",
      "41336.49\n",
      "41335.44\n",
      "41334.414\n",
      "41333.445\n",
      "41332.508\n",
      "41331.605\n",
      "41330.73\n",
      "41329.887\n",
      "41329.055\n",
      "41328.27\n",
      "41327.516\n",
      "41326.79\n",
      "41326.066\n",
      "41325.383\n",
      "41324.695\n",
      "41324.04\n",
      "41323.414\n",
      "41322.78\n",
      "41322.18\n",
      "41321.586\n",
      "41321.008\n",
      "41320.445\n",
      "41319.9\n",
      "41319.348\n",
      "41318.812\n",
      "41318.29\n",
      "41317.793\n",
      "41317.26\n",
      "41316.8\n",
      "41316.3\n",
      "41315.85\n",
      "41315.39\n",
      "41314.945\n",
      "41314.5\n",
      "Accuracy:  0.1823139851967277\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.13      0.16      0.15       354\n",
      "          1       0.00      0.00      0.00         1\n",
      "          2       0.01      0.50      0.02         6\n",
      "          3       0.00      0.00      0.00         0\n",
      "          4       0.12      0.17      0.14       310\n",
      "          5       0.72      0.19      0.30      1896\n",
      "\n",
      "avg / total       0.56      0.18      0.26      2567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aysa.fan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Read features\n",
    "def read_feature(file):\n",
    "  print(\"reading feature information...\\n\")\n",
    "  res = []\n",
    "  with open(file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "      line[i] = float(line[i])\n",
    "    res.append(line)\n",
    "  return np.array(res)\n",
    "\n",
    "# Read labels\n",
    "def read_label(file):\n",
    "  print(\"reading label information...\\n\")\n",
    "  res = []\n",
    "  with open(file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = int(line.strip())\n",
    "    res.append(line)\n",
    "  return np.array(res)\n",
    "\n",
    "def addLayer(inputData, inSize, outSize, activity_function = None):  \n",
    "    Weights = tf.Variable(tf.random_normal([inSize, outSize]))   \n",
    "    basis = tf.Variable(tf.random_uniform([1,outSize], -1, 1))    \n",
    "    weights_plus_b = tf.matmul(inputData, Weights) + basis  \n",
    "    #Wx_plus_b = tf.nn.dropout(weights_plus_b, keep_prob = 0.8)     # To prevent overfitting\n",
    "\n",
    "    if activity_function is None:  \n",
    "        ans = weights_plus_b  \n",
    "    else:  \n",
    "        ans = activity_function(weights_plus_b)\n",
    "    return ans  \n",
    "\n",
    "def net(x_data, y_data, x_test, y_test):\n",
    "    is_train = True\n",
    "\n",
    "\n",
    "    insize = x_data.shape[1]\n",
    "    outsize = 8\n",
    "    xs = tf.placeholder(tf.float32,[None, insize]) \n",
    "    ys = tf.placeholder(tf.float32,[None, outsize]) \n",
    "    keep_prob = tf.placeholder(tf.float32)  \n",
    "      \n",
    "    l1 = addLayer(xs, insize, 40,activity_function=None)  \n",
    "    l2 = addLayer(l1, 40, 20,activity_function=tf.nn.sigmoid)  \n",
    "    l3 = addLayer(l2, 20, 10,activity_function=tf.nn.softmax)  \n",
    "    l4 = addLayer(l3, 10, outsize,activity_function=tf.nn.softmax)\n",
    "\n",
    "\n",
    "    y = l4\n",
    "    #loss = tf.reduce_sum(tf.reduce_sum(tf.square((ys-l4)),reduction_indices = [1]))  \n",
    "    #loss = -tf.reduce_mean(ys * tf.log(l3))\n",
    "    #loss = tf.reduce_sum(tf.square((ys-y)))\n",
    "    loss = -tf.reduce_sum(ys * tf.log(y))\n",
    "    #loss = tf.reduce_sum(-tf.reduce_sum(ys * tf.log(y),reduction_indices=[1]))  # loss  \n",
    "    train =  tf.train.GradientDescentOptimizer(0.00001).minimize(loss) \n",
    "\n",
    "    # Turn 1 dimensional label vectors to 14 dimensional vectors which has only one element = 1\n",
    "    new_ydata = []\n",
    "    for i in range(y_data.shape[0]):\n",
    "      new_ydata.append([0]*outsize)\n",
    "      new_ydata[i][y_data[i]] = 1\n",
    "      # print(new_ydata[i])\n",
    "    new_ydata = np.array(new_ydata)\n",
    "        \n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        if is_train: \n",
    "            run_step = 4000\n",
    "            for i in range(run_step):  \n",
    "                sess.run(train,feed_dict={xs:x_data,ys:new_ydata})  \n",
    "                if i%50 == 0:  \n",
    "                    print(sess.run(loss,feed_dict={xs:x_data,ys:new_ydata}))\n",
    "            # save the model\n",
    "            saver=tf.train.Saver(max_to_keep=1)\n",
    "            saver.save(sess,'model/net.ckpt')\n",
    "        else:     # take a trained model\n",
    "            saver.restore(sess, 'model/net.ckpt')\n",
    "            print(\"save success!\")\n",
    "\n",
    "        # Prediction\n",
    "        res = sess.run(fetches=y, feed_dict={xs: x_test})\n",
    "        new_res = []\n",
    "        for ele in res:\n",
    "            mmax = -1111\n",
    "            index = -1\n",
    "            for i in range(outsize):\n",
    "                if ele[i] > mmax:\n",
    "                    index, mmax  = i, ele[i]\n",
    "            new_res.append(index)\n",
    "        #print(new_res)\n",
    "        new_res = np.array(new_res)\n",
    "        counter = 0\n",
    "        for i in range(len(new_res)):\n",
    "          if(y_test[i] == new_res[i]):\n",
    "            counter += 1\n",
    "        print(\"Accuracy: \", counter/len(new_res))\n",
    "        print(classification_report(new_res, y_test))\n",
    "\n",
    "def main():\n",
    "  feature = read_feature('data/feature.txt')\n",
    "  label = read_label('data/label.txt')\n",
    "\n",
    "  x_train , x_test , y_train , y_test = train_test_split(feature, label, test_size = 0.1,random_state=0)\n",
    "  net(x_train, y_train, x_test, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "2.5228014\n",
      "2.516215\n",
      "2.5097437\n",
      "2.5033638\n",
      "2.4970734\n",
      "2.4908638\n",
      "2.484738\n",
      "2.4786823\n",
      "2.4727101\n",
      "2.466812\n",
      "2.4609876\n",
      "2.4552302\n",
      "2.449538\n",
      "2.4439137\n",
      "2.4383428\n",
      "2.4328444\n",
      "2.4274163\n",
      "2.4220476\n",
      "2.416739\n",
      "2.4114907\n",
      "Accuracy:  0.15504479937670432\n"
     ]
    }
   ],
   "source": [
    "def read_feature(file):\n",
    "  print(\"reading feature information...\\n\")\n",
    "  res = []\n",
    "  with open(file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = line.split()\n",
    "    for i in range(len(line)):\n",
    "      line[i] = float(line[i])\n",
    "    res.append(line)\n",
    "  return np.array(res)\n",
    "\n",
    "def read_label(file):\n",
    "  print(\"reading label information...\\n\")\n",
    "  res = []\n",
    "  with open(file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = int(line.strip())\n",
    "    res.append(line)\n",
    "  return np.array(res)\n",
    "\n",
    "def addLayer(inputData, inSize, outSize, activity_function = None):  \n",
    "    Weights = tf.Variable(tf.random_normal([inSize, outSize]))   \n",
    "    basis = tf.Variable(tf.random_uniform([1,outSize], -1, 1))    \n",
    "    weights_plus_b = tf.matmul(inputData, Weights) + basis  \n",
    "    Wx_plus_b = tf.nn.dropout(weights_plus_b, keep_prob = 1)     # To prevent overfitting\n",
    "\n",
    "    if activity_function is None:  \n",
    "        ans = weights_plus_b  \n",
    "    else:  \n",
    "        ans = activity_function(weights_plus_b)\n",
    "    return ans  \n",
    "\n",
    "def net(x_data, y_data, x_test, y_test):\n",
    "    is_train = True\n",
    "\n",
    "\n",
    "    insize = x_data.shape[1]\n",
    "    outsize = 6\n",
    "    xs = tf.placeholder(tf.float32,[None, insize])   \n",
    "    ys = tf.placeholder(tf.float32,[None, outsize])  \n",
    "    keep_prob = tf.placeholder(tf.float32)  \n",
    "      \n",
    "    l1 = addLayer(xs, insize, 40,activity_function=tf.nn.sigmoid)  \n",
    "    l2 = addLayer(l1, 40, 20,activity_function=tf.nn.sigmoid)  \n",
    "    l3 = addLayer(l2, 20, 10,activity_function=tf.nn.sigmoid)  \n",
    "    l4 = addLayer(l3, 10, outsize,activity_function=tf.nn.softmax)\n",
    "    #l5 = addLayer(l4, 10, outsize,activity_function=tf.nn.softmax)\n",
    "\n",
    "\n",
    "    y = l4\n",
    "    #loss = tf.reduce_sum(tf.reduce_sum(tf.square((ys-l4)),reduction_indices = [1]))  \n",
    "    #loss = -tf.reduce_mean(ys * tf.log(l3))\n",
    "    #loss = tf.reduce_sum(tf.square((ys-y)))\n",
    "    #oss = -tf.reduce_sum(ys * tf.log(y))\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(y),reduction_indices=[1]))  # loss  \n",
    "    train =  tf.train.GradientDescentOptimizer(0.0001).minimize(loss) \n",
    "\n",
    "    new_ydata = []\n",
    "    for i in range(y_data.shape[0]):\n",
    "      new_ydata.append([0]*outsize)\n",
    "      new_ydata[i][y_data[i]] = 1\n",
    "      # print(new_ydata[i])\n",
    "    new_ydata = np.array(new_ydata)\n",
    "        \n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        if is_train: \n",
    "            run_step = 1000\n",
    "            for i in range(run_step):  \n",
    "                sess.run(train,feed_dict={xs:x_data,ys:new_ydata})  \n",
    "                if i%50 == 0:  \n",
    "                    print(sess.run(loss,feed_dict={xs:x_data,ys:new_ydata}))\n",
    "            # save the model\n",
    "            saver=tf.train.Saver(max_to_keep=1)\n",
    "            saver.save(sess,'model/net.ckpt')\n",
    "        else:     # use an existing model\n",
    "            saver.restore(sess, 'model/net.ckpt')\n",
    "            print(\"save success!\")\n",
    "\n",
    "        # Prediction\n",
    "        res = sess.run(fetches=y, feed_dict={xs: x_test})\n",
    "        new_res = []\n",
    "        for ele in res:\n",
    "            mmax = -1111\n",
    "            index = -1\n",
    "            for i in range(outsize):\n",
    "                if ele[i] > mmax:\n",
    "                    index, mmax  = i, ele[i]\n",
    "            new_res.append(index) \n",
    "        #print(new_res)\n",
    "        new_res = np.array(new_res)\n",
    "        counter = 0\n",
    "        for i in range(len(new_res)):\n",
    "          if (y_test[i] == new_res[i]):\n",
    "            counter += 1\n",
    "        #print(\"Accuracy: \", counter/len(new_res))\n",
    "        print(\"Accuracy: \", accuracy_score(y_test, new_res))\n",
    "\n",
    "def main():\n",
    "  #feature = read_feature('data/feature.txt')\n",
    "  #label = read_label('data/label.txt')\n",
    "\n",
    "  print(\"Loading data...\")\n",
    "  x_text, y = data_helpers.load_data_and_labels(\"data/sentence.txt\", \"data/label.txt\")\n",
    "\n",
    "  '''\n",
    "  outsize = 8\n",
    "  new_ydata = []\n",
    "  for i in range(len(y)):\n",
    "    new_ydata.append([0]*outsize)\n",
    "    new_ydata[i][y[i]] = 1\n",
    "    #print(new_ydata[i])\n",
    "  new_ydata = np.array(new_ydata)\n",
    "  y = new_ydata'''\n",
    "\n",
    "  # Build vocabulary\n",
    "  max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "  vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "  #feature = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "  #label = np.array(y)\n",
    "  x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "  #print(x.shape)\n",
    "  #print(max_document_length)\n",
    "\n",
    "  # Randomly shuffle data\n",
    "  np.random.seed(10)\n",
    "  shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "  feature = np.array(x)[shuffle_indices]\n",
    "  label = np.array(y)[shuffle_indices]\n",
    "\n",
    "  x_train , x_test , y_train , y_test = train_test_split(feature, label, test_size = 0.1)\n",
    "  net(x_train, y_train, x_test, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"feature_file\", \"data/sentence.txt\", \"feature data (sentence).\")\n",
    "tf.flags.DEFINE_string(\"label_file\", \"data/label.txt\", \"label data (number).\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"2,3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0001, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 120, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 1, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.feature_file, FLAGS.label_file)\n",
    "\n",
    "outsize = 6\n",
    "new_ydata = []\n",
    "for i in range(len(y)):\n",
    "  new_ydata.append([0]*outsize)\n",
    "  new_ydata[i][y[i]] = 1\n",
    "  #print(new_ydata[i])\n",
    "new_ydata = np.array(new_ydata)\n",
    "y = new_ydata\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "print(x.shape)\n",
    "print(max_document_length)\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "x_shuffled = np.array(x)[shuffle_indices]\n",
    "y_shuffled = np.array(y)[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "config = tf.ConfigProto(allow_soft_placement=True)          #my modification\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Graph().device('/gpu:3'):\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        #out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        #print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            #train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            #if writer:\n",
    "                #writer.add_summary(summaries, step)\n",
    "            return accuracy\n",
    "\n",
    "\n",
    "\n",
    "        is_train = False         \n",
    "\n",
    "        if is_train:\n",
    "            for kk in range(10):\n",
    "                # Generate batches\n",
    "                batches = data_helpers.batch_iter(\n",
    "                    list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "                # Training loop. For each batch...\n",
    "                for batch in batches:\n",
    "                    x_batch, y_batch = zip(*batch)\n",
    "                    train_step(x_batch, y_batch)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    if current_step % FLAGS.evaluate_every == 0:\n",
    "                        print(\"\\n\\n\\nEvaluation:\")\n",
    "                        test_acc = dev_step(x_dev, y_dev, writer=None)\n",
    "                        print(\"accuracy on test data is: {}\\n\\n\\n\".format(test_acc))\n",
    "                        saver.save(sess,'cnnmodel/net.ckpt')\n",
    "        else:\n",
    "            saver.restore(sess, 'cnnmodel/net.ckpt')\n",
    "            print(\"reload success!\")\n",
    "            test_acc = dev_step(x_dev, y_dev, writer=None)            \n",
    "            print(\"\\n\\nmodel accuracy on test data is: {}%\\n\\n\".format(test_acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textRNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TextRNN: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\n",
    "class TextRNN:\n",
    "    def __init__(self,num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,\n",
    "                 vocab_size,embed_size,is_training,initializer=tf.random_normal_initializer(stddev=0.1)):\n",
    "        \"\"\"init all hyperparameter here\"\"\"\n",
    "        # set hyperparamter\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length=sequence_length\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        self.hidden_size=embed_size\n",
    "        self.is_training=is_training\n",
    "        self.learning_rate=learning_rate\n",
    "        self.initializer=initializer\n",
    "        self.num_sampled=20\n",
    "\n",
    "\n",
    "        # add placeholder (X,label)\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")  # X\n",
    "        self.input_y = tf.placeholder(tf.int32,[None], name=\"input_y\")  # y [None,num_classes]\n",
    "        self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step=tf.Variable(0,trainable=False,name=\"Epoch_Step\")\n",
    "        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "\n",
    "        #print(self.input_y.shape)\n",
    "\n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference() #[None, self.label_size]. main computation graph is here.\n",
    "        if not is_training:\n",
    "            return\n",
    "        self.loss_val = self.loss() #-->self.loss_nce()\n",
    "        self.train_op = self.train()\n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")  # shape:[None,]\n",
    "        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y) #tf.argmax(self.logits, 1)-->[batch_size]\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\") # shape=()\n",
    "    def instantiate_weights(self):\n",
    "        \"\"\"define all weights here\"\"\"\n",
    "        with tf.name_scope(\"embedding\"): # embedding matrix\n",
    "            self.Embedding = tf.get_variable(\"Embedding\",shape=[self.vocab_size, self.embed_size],initializer=self.initializer) #[vocab_size,embed_size] tf.random_uniform([self.vocab_size, self.embed_size],-1.0,1.0)\n",
    "            self.W_projection = tf.get_variable(\"W_projection\",shape=[self.hidden_size*2, self.num_classes],initializer=self.initializer) #[embed_size,label_size]\n",
    "            self.b_projection = tf.get_variable(\"b_projection\",shape=[self.num_classes])       #[label_size]\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"main computation graph here: 1. embeddding layer, 2.Bi-LSTM layer, 3.concat, 4.FC layer 5.softmax \"\"\"\n",
    "        #1.get emebedding of words in the sentence\n",
    "        self.embedded_words = tf.nn.embedding_lookup(self.Embedding,self.input_x) #shape:[None,sentence_length,embed_size]\n",
    "        #2. Bi-lstm layer\n",
    "        # define lstm cess:get lstm cell output\n",
    "        lstm_fw_cell=rnn.BasicLSTMCell(self.hidden_size) #forward direction cell\n",
    "        lstm_bw_cell=rnn.BasicLSTMCell(self.hidden_size) #backward direction cell\n",
    "        if self.dropout_keep_prob is not None:\n",
    "            lstm_fw_cell=rnn.DropoutWrapper(lstm_fw_cell,output_keep_prob=self.dropout_keep_prob)\n",
    "            lstm_bw_cell=rnn.DropoutWrapper(lstm_bw_cell,output_keep_prob=self.dropout_keep_prob)\n",
    "        # bidirectional_dynamic_rnn: input: [batch_size, max_time, input_size]\n",
    "        #                            output: A tuple (outputs, output_states)\n",
    "        #                                    where:outputs: A tuple (output_fw, output_bw) containing the forward and the backward rnn output `Tensor`.\n",
    "        outputs,_=tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell,lstm_bw_cell,self.embedded_words,dtype=tf.float32) #[batch_size,sequence_length,hidden_size] #creates a dynamic bidirectional recurrent neural network\n",
    "        print(\"outputs:===>\",outputs) #outputs:(<tf.Tensor 'bidirectional_rnn/fw/fw/transpose:0' shape=(?, 5, 100) dtype=float32>, <tf.Tensor 'ReverseV2:0' shape=(?, 5, 100) dtype=float32>))\n",
    "        #3. concat output\n",
    "        output_rnn=tf.concat(outputs,axis=2) #[batch_size,sequence_length,hidden_size*2]\n",
    "        self.output_rnn_last=tf.reduce_mean(output_rnn,axis=1) #[batch_size,hidden_size*2] #output_rnn_last=output_rnn[:,-1,:] ##[batch_size,hidden_size*2] #TODO\n",
    "        print(\"output_rnn_last:\", self.output_rnn_last) # <tf.Tensor 'strided_slice:0' shape=(?, 200) dtype=float32>\n",
    "        #4. logits(use linear layer)\n",
    "        with tf.name_scope(\"output\"): #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "            logits = tf.matmul(self.output_rnn_last, self.W_projection) + self.b_projection  # [batch_size,num_classes]\n",
    "        return logits\n",
    "\n",
    "    def loss(self,l2_lambda=0.0001):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #input: `logits` and `labels` must have the same shape `[batch_size, num_classes]`\n",
    "            #output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits);#sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n",
    "            #print(\"1.sparse_softmax_cross_entropy_with_logits.losses:\",losses) # shape=(?,)\n",
    "            loss=tf.reduce_mean(losses)#print(\"2.loss.loss:\", loss) #shape=()\n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "            loss=loss+l2_losses\n",
    "        return loss\n",
    "\n",
    "    def loss_nce(self,l2_lambda=0.0001): #0.0001-->0.001\n",
    "        \"\"\"calculate loss using (NCE)cross entropy here\"\"\"\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        if self.is_training: #training\n",
    "            #labels=tf.reshape(self.input_y,[-1])               #[batch_size,1]------>[batch_size,]\n",
    "            labels=tf.expand_dims(self.input_y,1)                   #[batch_size,]----->[batch_size,1]\n",
    "            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                tf.nn.nce_loss(weights=tf.transpose(self.W_projection),#[hidden_size*2, num_classes]--->[num_classes,hidden_size*2]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n",
    "                               biases=self.b_projection,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n",
    "                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n",
    "                               inputs=self.output_rnn_last,# [batch_size,hidden_size*2] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                               num_sampled=self.num_sampled,  #scalar. 100\n",
    "                               num_classes=self.num_classes,partition_strategy=\"div\"))  #scalar. 1999\n",
    "        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "        loss = loss + l2_losses\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op\n",
    "\n",
    "#test started\n",
    "def test():\n",
    "    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n",
    "\n",
    "    tf.flags.DEFINE_string(\"feature_file\", \"data/sentence.txt\", \"feature data (sentence).\")\n",
    "    tf.flags.DEFINE_string(\"label_file\", \"data/label.txt\", \"label data (number).\")\n",
    "    tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\n",
    "    # Training parameters\n",
    "    tf.flags.DEFINE_integer(\"batch_size\", 256, \"Batch Size (default: 64)\")\n",
    "    tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "\n",
    "    FLAGS = tf.flags.FLAGS\n",
    "    FLAGS._parse_flags()\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(FLAGS.__flags.items()):\n",
    "        print(\"{}={}\".format(attr.upper(), value))\n",
    "    print(\"\")\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.feature_file, FLAGS.label_file)\n",
    "\n",
    "    y = np.array(y)\n",
    "\n",
    "    outsize = 6\n",
    "    new_ydata = []\n",
    "    for i in range(len(y)):\n",
    "      new_ydata.append([0]*outsize)\n",
    "      new_ydata[i][y[i]] = 1   \n",
    "      #print(new_ydata[i])\n",
    "    new_ydata = np.array(new_ydata)\n",
    "    new_y = new_ydata\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "    x_shuffled = np.array(x)[shuffle_indices]\n",
    "    y_shuffled = np.array(y)[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    num_classes=6\n",
    "    learning_rate=0.0001\n",
    "    batch_size=x_train.shape[0]\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=max_document_length\n",
    "    vocab_size=20570\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=0.5\n",
    "    textRNN=TextRNN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,vocab_size,embed_size,is_training)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)          #my modification\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    saver=tf.train.Saver()\n",
    "    is_train = False\n",
    "\n",
    "    with tf.Graph().device('/gpu:3'), tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if is_train:\n",
    "            for kk in range(30):\n",
    "                # Generate batches\n",
    "                batches = data_helpers.batch_iter(\n",
    "                    list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "                step = 0\n",
    "                # Training loop. For each batch...\n",
    "                tmp_batches = batches\n",
    "                for batch in tmp_batches:\n",
    "                    x_batch, y_batch = zip(*batch)\n",
    "                    #train_step(x_batch, y_batch)\n",
    "                    input_x = x_batch\n",
    "                    input_y = y_batch\n",
    "                    loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:input_x,textRNN.input_y:input_y,textRNN.dropout_keep_prob:dropout_keep_prob})\n",
    "                    print(\"iteration\", kk, \"step\", step, \"loss:\",loss,\"acc:\",acc)#\"label:\",input_y,\"prediction:\",predict)\n",
    "                    step += 1\n",
    "                    if step%100 == 0:\n",
    "                        loss,acc,predict,_=sess.run([textRNN.loss_val,textRNN.accuracy,textRNN.predictions,textRNN.train_op],feed_dict={textRNN.input_x:x_dev,textRNN.input_y:y_dev,textRNN.dropout_keep_prob:dropout_keep_prob})\n",
    "                        print(\"**********************************\", \"iteration\", kk, \"step\", step, \"loss:\",loss,\"acc:\",acc)#\"label:\",input_y,\"prediction:\",predict)\n",
    "\n",
    "                        saver=tf.train.Saver(max_to_keep=1)\n",
    "                        saver.save(sess,'rnnmodel/net.ckpt')\n",
    "        else:\n",
    "            saver.restore(sess, 'rnnmodel/net.ckpt')\n",
    "            print(\"reload success!\")\n",
    "            loss, acc, predict,_=sess.run([textRNN.loss_val, textRNN.accuracy, textRNN.predictions, textRNN.train_op],feed_dict={textRNN.input_x:x_dev,textRNN.input_y:y_dev,textRNN.dropout_keep_prob:1})\n",
    "            print(\"\\n\\nmodel accuracy on test data is: {}%\\n\\n\".format(acc*100))\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The baseline we used for the classification is random guessing. Since there are 6 characters, the probability of correctly guessing a classification is 1/6 which is 16.67%. The accuracies for our modelings are as below:**\n",
    "\n",
    "* word2vec model and DNN model (with accuracy of 18%~19%)\n",
    "* CNN model (with acuracy of 28%~30%)\n",
    "* LSTM model (with accuracy of 39%~40%)\n",
    "\n",
    "**Apparently, LSTM model has the best accuracy for character classification. It is not as high as we expected, but it is still (much) higher than random guessing. The main issue of the low accuracy is that we only have fewer than 20,000 lines of sentences for the 6 main characters, and there is no way to gather more data because the show is over. Also, the lines from a show are a good representative of the natural language, but definitely not the natural enough: for example, people tend to use the same sentences over and over again in reality, but a show can't have too many same lines for a character or the audience gets bored. We are satisfied with the results so far.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We are still in the early stage of the language drift part of our project, and that part will be included in the final paper.**\n",
    "**Different from character classification, we can gather more data from the shows during the same period of time to improve the data size, and that may help with the final results.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
