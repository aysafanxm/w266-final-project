{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Sitcoms can serve as a good corpus for language studies despite their relatively smaller size due to the \n",
    "\n",
    "In this paper we model the character classification and a simple version of language drift based on a popular TV show \"Friends\" as well as other shows running in the same era. The modeling methods applied in this paer include word2vec, DNN, CNN and LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Our underlying assumption is that sitcoms are a good case study for natural language for two main reasons. First is distinctive characters with recoginizable, often exaggerated speaking styles and verbal tics (often in the form of character catchphrases). This may enable a form of author detection using both a smaller corpus of words, and smaller sample of text. In the real world, there may be many author detection tasks that fall somewhere between trying to determine which Federalist Paper was written by Alexander Hamilton, and trying to determine whether it was Samantha, Carrie, Charlotte, or Miranda who said a particular line. Second, sitcoms will try to adhere to the current zeitgeist and popular culture, incorporating newer pieces of popular slang and phraseology into dialogue, though often in a hamfisted manner, when the characters were younger, or ironically, if the characters were older. We believe that because of this, sitcom dialogue can serve as a model for language drift in wider society.\n",
    "\n",
    "<img src=\"friends-reunion-series-ftr.jpg\" style=\"width: 400px;\"> \n",
    "\n",
    "*Friends* was one of the most popular tv series' in the world during its run of 226 episodes from 1994 to 2004. Because of its long run, distinctive, but not hyperbolic characters, and snappy, but still believable dialogue, we believe that the show is perfect for this type of study.\n",
    "\n",
    "The ten year run of the show allows for some study of linguistic drift, as we believe the writers intended for the character dialogue to sound current. The distinct characters allows us to perform a character identification task. In some ways, the two tasks may interefere with each other as change in language might be due to character development rather than language drift in broader society. Character development can also make the character classification tasks more difficult.\n",
    "\n",
    "*A \"Friends\" icon painting by Aysa:*\n",
    "\n",
    "<img src=\"friends_painting.jpg\" style=\"width: 200px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Cristian Danescu-Niculescu-Mizil, Robert West, Dan Jurafsky, Jure Leskovec, and Christopher Potts proposed a framework for studying linguistic drift in their 2013 paper, *No Country for Old Member*. In it, they studied two online communities with a total of more than 4 million posts of a median of more than 50 words, each. They created a snapshot language model for each month in the community by creating a bigram language model with Katz backoff smoothing, and then comparing each post to the model to find how surprising it's language was with the language of the community of the time by calculating corss-entropy. The paper also examined the frequency of certain types of words over time.\n",
    "\n",
    "Dario Bertero and Pascale Fung, in their 2016 paper, *A Long Short-Term Memory Framework for Predicting Humor in Dialogues* succesfully used an LSTM model to predict the setup-punchline structure of sitcom dialogue. They also experimented with a convolutional neural network using character trigrams, word2vec, and word tokens as input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Character identification\n",
    "* Modeling using word2vec, DNN, CNN and LSTM models, the folders and scripts are shown as below. We are still looking to further improve the accuracy, if it's possible. \n",
    "* The baseline accuracy would be random guessing, which has an accuracy of 1/6, or 16.67%. If we get a higher accuracy in our modeling, the model is (at least somewhat) useful!\n",
    "* We have attched part of the codes in this report, the complete version of all codes will be uploaded into this repository: https://github.com/aysafanxm/w266-final-project\n",
    "## Language Drift\n",
    "Our corpus is considerably smaller than in Danescu et. al, with only 28,876 lines of dialogue. Instead of using computing a monthly snapshot language model, we will instead measure drift season by season.\n",
    "\n",
    "\n",
    "### Folders\n",
    "* Model folder: word2vec model and DNN model\n",
    "* Cnnmodel folder: CNN model\n",
    "* Rnnmodel folder: LSTM model\n",
    "\n",
    "\n",
    "### Scripts\n",
    "* *handle_json.py* - The original lines.json file contains all lines from the show \"Friends\", which was in JSON format, we converted the character names into numbers and write them into data/feature_raw.txt. We also pick only the 6 main characters' lines (6 main characters: Ross, Rachel, Joey, Chandler, Monica and Phoebe).\n",
    "\n",
    "\n",
    "* *extract_label_and_sentence.py* - Extract labels from data/feature_raw.txt and write them into data/label.txt, also extract the segmantations into data/sentence.txt.\n",
    "\n",
    "\n",
    "* *extract_feature.py* - Train word vectors using word2vec (4 dimensions) and calculate the feature vectors of each sentence (take the average of the word vectors in each sentence), then write feature vectors into data/feature.txt. This process is mainly for DNN training because CNN and LSTM use embedding which doesn't train word vector the same way.\n",
    "\n",
    "\n",
    "* *main_word2vec.py* - DNN with 3 hidden layers. The neuron numbers of each layer is 40, 20 and 10, respectively. The input dimension is 4 (4 features) and the output dimension is 6 (6 characters). The first 2 layersâ€˜ activation function is sigmoid and the last layer's is softmax. The learning rate is 0.0001 and there are 1000 iterations. Note that there is a parameter, is_train, in the model, if is_train is True, it starts to train a new model, otherwise it takes the trained model.\n",
    "\n",
    "\n",
    "* *main.py* - Similar to main_word2vec.py, but it is DNN with embedding.\n",
    "\n",
    "\n",
    "* *data_helpers.py* - Helps to batch process the data\n",
    "\n",
    "\n",
    "* *cnn_model.py* - CNN with an embedding layer (100 dimensions word vectos), a CNN layer, a pool layer and a softmax layer to output the probability of each label.\n",
    "\n",
    "\n",
    "* *textCNN.py* - It takes the cnn_model.py to train or test the lines data. It takes 90% of the lines for training and 10% of them for testing. *The accuracy of CNN is 28%~30%.* The learn rate is 0.0001.\n",
    "\n",
    "\n",
    "* *textRNN.py* - RNN with an embedding layer, a bi-lstm layer, a concat layer, a fully connected layer and a softmax layer. It takes 90% of the lines for training and 10% of them for testing. *The accuracy of RNN is 39%~40%.* The learn rate is 0.0001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The baseline we used for the classification is random guessing. Since there are 6 characters, the probability of correctly guessing a classification is 1/6 which is 16.67%. The accuracies for our modelings are as below:**\n",
    "\n",
    "* word2vec model and DNN model (with accuracy of 18%~19%)\n",
    "* CNN model (with acuracy of 28%~30%)\n",
    "* LSTM model (with accuracy of 39%~40%)\n",
    "\n",
    "**Apparently, LSTM model has the best accuracy for character classification. It is not as high as we expected, but it is still (much) higher than random guessing. The main issue of the low accuracy is that we only have fewer than 20,000 lines of sentences for the 6 main characters, and there is no way to gather more data because the show is over. Also, the lines from a show are a good representative of the natural language, but definitely not the natural enough: for example, people tend to use the same sentences over and over again in reality, but a show can't have too many same lines for a character or the audience gets bored. We are satisfied with the results so far.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We are still in the early stage of the language drift part of our project, and that part will be included in the final paper.**\n",
    "**Different from character classification, we can gather more data from the shows during the same period of time to improve the data size, and that may help with the final results.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
